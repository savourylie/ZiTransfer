{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loaders(root, batch_size, valid_size=0.1, option='cifar10', cuda=False, random_seed=1113):\n",
    "    # Prepare datasets\n",
    "    kwargs = {'num_workers': 8, 'pin_memory': True} if torch.cuda.is_available() and cuda else {}\n",
    "\n",
    "    if option == 'cifar10':\n",
    "        # Load CIFAR10 dataset\n",
    "        transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "                                              transforms.RandomHorizontalFlip(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "                                              Cutout(n_holes=1, length=16)\n",
    "                                              ])\n",
    "\n",
    "        transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "                                             ])\n",
    "\n",
    "        training_set = datasets.CIFAR10(root=root, train=True, download=True, transform=transform_train)\n",
    "        validation_set = datasets.CIFAR10(root=root, train=True, transform=transform_test)\n",
    "        test_set = datasets.CIFAR10(root=root, train=False, transform=transform_test)\n",
    "\n",
    "        # classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    elif option == 'mnist':\n",
    "        # Load MNIST\n",
    "        transform_train = transforms.Compose([transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "#                                               transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                              ])\n",
    "\n",
    "        transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "#                                              transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                             ])\n",
    "\n",
    "        training_set = datasets.MNIST(root=root, train=True, download=True, transform=transform_train)\n",
    "        validation_set = datasets.MNIST(root=root, train=True, transform=transform_test)\n",
    "        test_set = datasets.MNIST(root=root, train=False, transform=transform_test)\n",
    "        # classes = tuple(range(10))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Data option not supported.\")\n",
    "\n",
    "    full_train_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return full_train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/\"\n",
    "\n",
    "\n",
    "train_loader, test_loader = data_loaders(root=data_path, batch_size=100, valid_size=0.3,\n",
    "                                   option='mnist', cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(100, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 784)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.fc4(x)\n",
    "        x = F.tanh(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = self.fc4(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, data_loader, epochs=100, log_interval=10):\n",
    "    if torch.cuda.is_available():\n",
    "        generator = generator.cuda()\n",
    "        discriminator = discriminator.cuda()\n",
    "        \n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=lr)\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr / 10)\n",
    "        \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for batch_idx, (data, _) in enumerate(train_loader, 1):\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                \n",
    "            batch_size = data.shape[0]\n",
    "            \n",
    "            z_input = torch.randn(batch_size, 100)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                z_input = z_input.cuda()\n",
    "                \n",
    "            g_output = generator(z_input)\n",
    "        \n",
    "            real_labels = torch.ones(batch_size)\n",
    "            fake_labels = torch.zeros(batch_size)    \n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                real_labels = real_labels.cuda()\n",
    "                fake_labels = fake_labels.cuda()\n",
    "\n",
    "            d_optimizer.zero_grad()\n",
    "            d_output_fake = discriminator(g_output)\n",
    "            d_output_real = discriminator(data.view(-1, data.size(1) * data.size(2) * data.size(3)))\n",
    "            \n",
    "            fake_loss =  criterion(d_output_fake, fake_labels)\n",
    "            real_loss = criterion(d_output_real, real_labels)\n",
    "            \n",
    "            d_loss = fake_loss + real_loss\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            g_optimizer.zero_grad()\n",
    "            g_loss = criterion(d_output_fake, real_labels)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "                    \n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [Iter: {}]\\t Gloss: {:.6f} Dloss: {:.6f}'.format(\n",
    "                      epoch, batch_idx, g_loss.data.item(), d_loss.data.item()))\n",
    "                    \n",
    "                \n",
    "        if not epoch % log_interval:\n",
    "            images_torch = g_output.view(-1, 1, 28, 28)\n",
    "            images = images_torch.cpu().detach().numpy()\n",
    "            img = images[0]\n",
    "            print(img.shape)\n",
    "            img = np.rollaxis(img, 0, 3).squeeze(2)                       \n",
    "            print(img.shape)\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = Generator()\n",
    "d_model = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [Iter: 100]\t Gloss: 2.055814 Dloss: 0.227065\n",
      "Train Epoch: 1 [Iter: 200]\t Gloss: 1.006774 Dloss: 0.527398\n",
      "Train Epoch: 1 [Iter: 300]\t Gloss: 1.318467 Dloss: 0.420081\n",
      "Train Epoch: 1 [Iter: 400]\t Gloss: 1.793002 Dloss: 0.222515\n",
      "Train Epoch: 1 [Iter: 500]\t Gloss: 3.418005 Dloss: 0.058249\n",
      "Train Epoch: 1 [Iter: 600]\t Gloss: 3.161048 Dloss: 0.079999\n",
      "Train Epoch: 2 [Iter: 100]\t Gloss: 3.102488 Dloss: 0.091298\n",
      "Train Epoch: 2 [Iter: 200]\t Gloss: 5.376804 Dloss: 0.008862\n",
      "Train Epoch: 2 [Iter: 300]\t Gloss: 5.709061 Dloss: 0.005479\n",
      "Train Epoch: 2 [Iter: 400]\t Gloss: 2.867065 Dloss: 0.079802\n",
      "Train Epoch: 2 [Iter: 500]\t Gloss: 6.597619 Dloss: 0.005034\n",
      "Train Epoch: 2 [Iter: 600]\t Gloss: 7.198427 Dloss: 0.002518\n",
      "Train Epoch: 3 [Iter: 100]\t Gloss: 7.674338 Dloss: 0.001335\n",
      "Train Epoch: 3 [Iter: 200]\t Gloss: 7.982672 Dloss: 0.001078\n",
      "Train Epoch: 3 [Iter: 300]\t Gloss: 8.266235 Dloss: 0.000787\n",
      "Train Epoch: 3 [Iter: 400]\t Gloss: 8.446899 Dloss: 0.000607\n",
      "Train Epoch: 3 [Iter: 500]\t Gloss: 8.607833 Dloss: 0.000499\n",
      "Train Epoch: 3 [Iter: 600]\t Gloss: 8.749572 Dloss: 0.000429\n",
      "Train Epoch: 4 [Iter: 100]\t Gloss: 8.925294 Dloss: 0.000376\n",
      "Train Epoch: 4 [Iter: 200]\t Gloss: 9.096740 Dloss: 0.000295\n",
      "Train Epoch: 4 [Iter: 300]\t Gloss: 9.176923 Dloss: 0.000395\n",
      "Train Epoch: 4 [Iter: 400]\t Gloss: 9.327402 Dloss: 0.000243\n",
      "Train Epoch: 4 [Iter: 500]\t Gloss: 9.458850 Dloss: 0.000198\n",
      "Train Epoch: 4 [Iter: 600]\t Gloss: 9.518878 Dloss: 0.000223\n",
      "Train Epoch: 5 [Iter: 100]\t Gloss: 9.661347 Dloss: 0.000164\n",
      "Train Epoch: 5 [Iter: 200]\t Gloss: 9.792630 Dloss: 0.000169\n",
      "Train Epoch: 5 [Iter: 300]\t Gloss: 9.892766 Dloss: 0.000129\n",
      "Train Epoch: 5 [Iter: 400]\t Gloss: 9.844666 Dloss: 0.000166\n",
      "Train Epoch: 5 [Iter: 500]\t Gloss: 10.005068 Dloss: 0.000142\n",
      "Train Epoch: 5 [Iter: 600]\t Gloss: 10.080387 Dloss: 0.000114\n",
      "Train Epoch: 6 [Iter: 100]\t Gloss: 10.207655 Dloss: 0.000117\n",
      "Train Epoch: 6 [Iter: 200]\t Gloss: 10.315690 Dloss: 0.000092\n",
      "Train Epoch: 6 [Iter: 300]\t Gloss: 10.408259 Dloss: 0.000088\n",
      "Train Epoch: 6 [Iter: 400]\t Gloss: 10.483420 Dloss: 0.000086\n",
      "Train Epoch: 6 [Iter: 500]\t Gloss: 10.583416 Dloss: 0.000086\n",
      "Train Epoch: 6 [Iter: 600]\t Gloss: 10.670789 Dloss: 0.000079\n",
      "Train Epoch: 7 [Iter: 100]\t Gloss: 10.759579 Dloss: 0.000061\n",
      "Train Epoch: 7 [Iter: 200]\t Gloss: 10.824466 Dloss: 0.000062\n",
      "Train Epoch: 7 [Iter: 300]\t Gloss: 10.914870 Dloss: 0.000051\n",
      "Train Epoch: 7 [Iter: 400]\t Gloss: 11.008445 Dloss: 0.000049\n",
      "Train Epoch: 7 [Iter: 500]\t Gloss: 11.088748 Dloss: 0.000045\n",
      "Train Epoch: 7 [Iter: 600]\t Gloss: 11.164350 Dloss: 0.000038\n",
      "Train Epoch: 8 [Iter: 100]\t Gloss: 11.215319 Dloss: 0.000034\n",
      "Train Epoch: 8 [Iter: 200]\t Gloss: 11.304155 Dloss: 0.000033\n",
      "Train Epoch: 8 [Iter: 300]\t Gloss: 11.385380 Dloss: 0.000038\n",
      "Train Epoch: 8 [Iter: 400]\t Gloss: 11.460314 Dloss: 0.000033\n",
      "Train Epoch: 8 [Iter: 500]\t Gloss: 11.534628 Dloss: 0.000032\n",
      "Train Epoch: 8 [Iter: 600]\t Gloss: 11.603324 Dloss: 0.000066\n",
      "Train Epoch: 9 [Iter: 100]\t Gloss: 11.668690 Dloss: 0.000028\n",
      "Train Epoch: 9 [Iter: 200]\t Gloss: 11.722655 Dloss: 0.000022\n",
      "Train Epoch: 9 [Iter: 300]\t Gloss: 11.797772 Dloss: 0.000022\n",
      "Train Epoch: 9 [Iter: 400]\t Gloss: 11.869331 Dloss: 0.000026\n",
      "Train Epoch: 9 [Iter: 500]\t Gloss: 11.940747 Dloss: 0.000027\n",
      "Train Epoch: 9 [Iter: 600]\t Gloss: 12.005115 Dloss: 0.000019\n",
      "Train Epoch: 10 [Iter: 100]\t Gloss: 12.048695 Dloss: 0.000016\n",
      "Train Epoch: 10 [Iter: 200]\t Gloss: 12.129843 Dloss: 0.000018\n",
      "Train Epoch: 10 [Iter: 300]\t Gloss: 12.196667 Dloss: 0.000014\n",
      "Train Epoch: 10 [Iter: 400]\t Gloss: 12.245461 Dloss: 0.000019\n",
      "Train Epoch: 10 [Iter: 500]\t Gloss: 12.307018 Dloss: 0.000013\n",
      "Train Epoch: 10 [Iter: 600]\t Gloss: 12.371227 Dloss: 0.000011\n",
      "(1, 28, 28)\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [Iter: 100]\t Gloss: 7.078351 Dloss: 0.005700\n",
      "Train Epoch: 11 [Iter: 200]\t Gloss: 5.800197 Dloss: 0.003517\n",
      "Train Epoch: 11 [Iter: 300]\t Gloss: 7.876993 Dloss: 0.000492\n",
      "Train Epoch: 11 [Iter: 400]\t Gloss: 7.728708 Dloss: 0.003442\n",
      "Train Epoch: 11 [Iter: 500]\t Gloss: 8.887798 Dloss: 0.000163\n",
      "Train Epoch: 11 [Iter: 600]\t Gloss: 6.243179 Dloss: 0.003290\n",
      "Train Epoch: 12 [Iter: 100]\t Gloss: 10.429129 Dloss: 0.000059\n",
      "Train Epoch: 12 [Iter: 200]\t Gloss: 7.746459 Dloss: 0.000517\n",
      "Train Epoch: 12 [Iter: 300]\t Gloss: 8.664832 Dloss: 0.000307\n",
      "Train Epoch: 12 [Iter: 400]\t Gloss: 9.164437 Dloss: 0.000152\n",
      "Train Epoch: 12 [Iter: 500]\t Gloss: 9.879733 Dloss: 0.000104\n",
      "Train Epoch: 12 [Iter: 600]\t Gloss: 10.053699 Dloss: 0.000126\n",
      "Train Epoch: 13 [Iter: 100]\t Gloss: 9.791146 Dloss: 0.000072\n",
      "Train Epoch: 13 [Iter: 200]\t Gloss: 10.720203 Dloss: 0.000039\n",
      "Train Epoch: 13 [Iter: 300]\t Gloss: 10.661018 Dloss: 0.000066\n",
      "Train Epoch: 13 [Iter: 400]\t Gloss: 8.972901 Dloss: 0.000183\n",
      "Train Epoch: 13 [Iter: 500]\t Gloss: 11.016732 Dloss: 0.000050\n",
      "Train Epoch: 13 [Iter: 600]\t Gloss: 11.391687 Dloss: 0.000041\n",
      "Train Epoch: 14 [Iter: 100]\t Gloss: 11.714765 Dloss: 0.000013\n",
      "Train Epoch: 14 [Iter: 200]\t Gloss: 12.015455 Dloss: 0.000031\n",
      "Train Epoch: 14 [Iter: 300]\t Gloss: 12.154900 Dloss: 0.000013\n",
      "Train Epoch: 14 [Iter: 400]\t Gloss: 12.462459 Dloss: 0.000007\n",
      "Train Epoch: 14 [Iter: 500]\t Gloss: 12.637254 Dloss: 0.000008\n",
      "Train Epoch: 14 [Iter: 600]\t Gloss: 12.728284 Dloss: 0.000010\n",
      "Train Epoch: 15 [Iter: 100]\t Gloss: 12.838382 Dloss: 0.000007\n",
      "Train Epoch: 15 [Iter: 200]\t Gloss: 12.959181 Dloss: 0.000005\n",
      "Train Epoch: 15 [Iter: 300]\t Gloss: 13.075845 Dloss: 0.000005\n",
      "Train Epoch: 15 [Iter: 400]\t Gloss: 13.236048 Dloss: 0.000005\n",
      "Train Epoch: 15 [Iter: 500]\t Gloss: 13.249746 Dloss: 0.000005\n",
      "Train Epoch: 15 [Iter: 600]\t Gloss: 13.349332 Dloss: 0.000003\n",
      "Train Epoch: 16 [Iter: 100]\t Gloss: 13.463992 Dloss: 0.000003\n",
      "Train Epoch: 16 [Iter: 200]\t Gloss: 13.559125 Dloss: 0.000005\n",
      "Train Epoch: 16 [Iter: 300]\t Gloss: 13.645507 Dloss: 0.000003\n",
      "Train Epoch: 16 [Iter: 400]\t Gloss: 13.550824 Dloss: 0.000003\n",
      "Train Epoch: 16 [Iter: 500]\t Gloss: 13.691772 Dloss: 0.000004\n",
      "Train Epoch: 16 [Iter: 600]\t Gloss: 13.790684 Dloss: 0.000002\n",
      "Train Epoch: 17 [Iter: 100]\t Gloss: 13.890791 Dloss: 0.000002\n",
      "Train Epoch: 17 [Iter: 200]\t Gloss: 13.987268 Dloss: 0.000002\n",
      "Train Epoch: 17 [Iter: 300]\t Gloss: 14.075109 Dloss: 0.000002\n",
      "Train Epoch: 17 [Iter: 400]\t Gloss: 14.130528 Dloss: 0.000002\n",
      "Train Epoch: 17 [Iter: 500]\t Gloss: 14.190639 Dloss: 0.000002\n",
      "Train Epoch: 17 [Iter: 600]\t Gloss: 14.267697 Dloss: 0.000002\n",
      "Train Epoch: 18 [Iter: 100]\t Gloss: 14.340687 Dloss: 0.000003\n",
      "Train Epoch: 18 [Iter: 200]\t Gloss: 14.398074 Dloss: 0.000002\n",
      "Train Epoch: 18 [Iter: 300]\t Gloss: 14.468261 Dloss: 0.000001\n",
      "Train Epoch: 18 [Iter: 400]\t Gloss: 14.545685 Dloss: 0.000001\n",
      "Train Epoch: 18 [Iter: 500]\t Gloss: 14.018787 Dloss: 0.000002\n",
      "Train Epoch: 18 [Iter: 600]\t Gloss: 11.385951 Dloss: 0.000013\n",
      "Train Epoch: 19 [Iter: 100]\t Gloss: 11.990795 Dloss: 0.000786\n",
      "Train Epoch: 19 [Iter: 200]\t Gloss: 12.386486 Dloss: 0.000007\n",
      "Train Epoch: 19 [Iter: 300]\t Gloss: 13.331505 Dloss: 0.000003\n",
      "Train Epoch: 19 [Iter: 400]\t Gloss: 13.442867 Dloss: 0.000002\n",
      "Train Epoch: 19 [Iter: 500]\t Gloss: 13.737461 Dloss: 0.000003\n",
      "Train Epoch: 19 [Iter: 600]\t Gloss: 13.955652 Dloss: 0.000002\n",
      "Train Epoch: 20 [Iter: 100]\t Gloss: 14.219151 Dloss: 0.000003\n",
      "Train Epoch: 20 [Iter: 200]\t Gloss: 14.349521 Dloss: 0.000001\n",
      "Train Epoch: 20 [Iter: 300]\t Gloss: 14.511106 Dloss: 0.000001\n",
      "Train Epoch: 20 [Iter: 400]\t Gloss: 14.682854 Dloss: 0.000001\n",
      "Train Epoch: 20 [Iter: 500]\t Gloss: 14.789150 Dloss: 0.000003\n",
      "Train Epoch: 20 [Iter: 600]\t Gloss: 14.885255 Dloss: 0.000001\n",
      "(1, 28, 28)\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21 [Iter: 100]\t Gloss: 14.991366 Dloss: 0.000001\n",
      "Train Epoch: 21 [Iter: 200]\t Gloss: 15.059561 Dloss: 0.000001\n",
      "Train Epoch: 21 [Iter: 300]\t Gloss: 15.183512 Dloss: 0.000000\n",
      "Train Epoch: 21 [Iter: 400]\t Gloss: 15.286868 Dloss: 0.000001\n",
      "Train Epoch: 21 [Iter: 500]\t Gloss: 15.361685 Dloss: 0.000001\n",
      "Train Epoch: 21 [Iter: 600]\t Gloss: 14.861195 Dloss: 0.000001\n",
      "Train Epoch: 22 [Iter: 100]\t Gloss: 15.121793 Dloss: 0.000000\n",
      "Train Epoch: 22 [Iter: 200]\t Gloss: 14.910320 Dloss: 0.000001\n",
      "Train Epoch: 22 [Iter: 300]\t Gloss: 14.957775 Dloss: 0.000000\n",
      "Train Epoch: 22 [Iter: 400]\t Gloss: 10.753169 Dloss: 0.000211\n",
      "Train Epoch: 22 [Iter: 500]\t Gloss: 19.354639 Dloss: 0.000002\n",
      "Train Epoch: 22 [Iter: 600]\t Gloss: 18.562376 Dloss: 0.000001\n",
      "Train Epoch: 23 [Iter: 100]\t Gloss: 18.126472 Dloss: 0.000000\n",
      "Train Epoch: 23 [Iter: 200]\t Gloss: 17.869846 Dloss: 0.000000\n",
      "Train Epoch: 23 [Iter: 300]\t Gloss: 17.666872 Dloss: 0.000000\n",
      "Train Epoch: 23 [Iter: 400]\t Gloss: 17.523317 Dloss: 0.000000\n",
      "Train Epoch: 23 [Iter: 500]\t Gloss: 17.397219 Dloss: 0.000000\n",
      "Train Epoch: 23 [Iter: 600]\t Gloss: 17.271233 Dloss: 0.000000\n",
      "Train Epoch: 24 [Iter: 100]\t Gloss: 17.205286 Dloss: 0.000000\n",
      "Train Epoch: 24 [Iter: 200]\t Gloss: 16.908699 Dloss: 0.000000\n",
      "Train Epoch: 24 [Iter: 300]\t Gloss: 16.874146 Dloss: 0.000000\n",
      "Train Epoch: 24 [Iter: 400]\t Gloss: 16.852016 Dloss: 0.000000\n",
      "Train Epoch: 24 [Iter: 500]\t Gloss: 16.847893 Dloss: 0.000000\n",
      "Train Epoch: 24 [Iter: 600]\t Gloss: 16.844397 Dloss: 0.000000\n",
      "Train Epoch: 25 [Iter: 100]\t Gloss: 16.854404 Dloss: 0.000000\n",
      "Train Epoch: 25 [Iter: 200]\t Gloss: 16.870518 Dloss: 0.000000\n",
      "Train Epoch: 25 [Iter: 300]\t Gloss: 16.898710 Dloss: 0.000000\n",
      "Train Epoch: 25 [Iter: 400]\t Gloss: 16.923332 Dloss: 0.000000\n",
      "Train Epoch: 25 [Iter: 500]\t Gloss: 16.958582 Dloss: 0.000000\n",
      "Train Epoch: 25 [Iter: 600]\t Gloss: 16.992647 Dloss: 0.000000\n",
      "Train Epoch: 26 [Iter: 100]\t Gloss: 17.038977 Dloss: 0.000000\n",
      "Train Epoch: 26 [Iter: 200]\t Gloss: 17.069221 Dloss: 0.000000\n",
      "Train Epoch: 26 [Iter: 300]\t Gloss: 17.100012 Dloss: 0.000000\n",
      "Train Epoch: 26 [Iter: 400]\t Gloss: 17.140659 Dloss: 0.000000\n",
      "Train Epoch: 26 [Iter: 500]\t Gloss: 17.180874 Dloss: 0.000000\n",
      "Train Epoch: 26 [Iter: 600]\t Gloss: 17.225256 Dloss: 0.000000\n",
      "Train Epoch: 27 [Iter: 100]\t Gloss: 17.267708 Dloss: 0.000000\n",
      "Train Epoch: 27 [Iter: 200]\t Gloss: 17.305918 Dloss: 0.000000\n",
      "Train Epoch: 27 [Iter: 300]\t Gloss: 17.342894 Dloss: 0.000000\n",
      "Train Epoch: 27 [Iter: 400]\t Gloss: 17.399084 Dloss: 0.000000\n",
      "Train Epoch: 27 [Iter: 500]\t Gloss: 17.446047 Dloss: 0.000000\n",
      "Train Epoch: 27 [Iter: 600]\t Gloss: 17.476595 Dloss: 0.000000\n",
      "Train Epoch: 28 [Iter: 100]\t Gloss: 17.535618 Dloss: 0.000000\n",
      "Train Epoch: 28 [Iter: 200]\t Gloss: 17.569178 Dloss: 0.000000\n",
      "Train Epoch: 28 [Iter: 300]\t Gloss: 17.615694 Dloss: 0.000000\n",
      "Train Epoch: 28 [Iter: 400]\t Gloss: 17.662502 Dloss: 0.000000\n",
      "Train Epoch: 28 [Iter: 500]\t Gloss: 17.704800 Dloss: 0.000000\n",
      "Train Epoch: 28 [Iter: 600]\t Gloss: 17.750969 Dloss: 0.000000\n",
      "Train Epoch: 29 [Iter: 100]\t Gloss: 17.800901 Dloss: 0.000000\n",
      "Train Epoch: 29 [Iter: 200]\t Gloss: 17.852995 Dloss: 0.000000\n",
      "Train Epoch: 29 [Iter: 300]\t Gloss: 17.905554 Dloss: 0.000000\n",
      "Train Epoch: 29 [Iter: 400]\t Gloss: 17.937864 Dloss: 0.000000\n",
      "Train Epoch: 29 [Iter: 500]\t Gloss: 17.979036 Dloss: 0.000000\n",
      "Train Epoch: 29 [Iter: 600]\t Gloss: 18.031157 Dloss: 0.000000\n",
      "Train Epoch: 30 [Iter: 100]\t Gloss: 17.638191 Dloss: 0.000000\n",
      "Train Epoch: 30 [Iter: 200]\t Gloss: 17.801634 Dloss: 0.000000\n",
      "Train Epoch: 30 [Iter: 300]\t Gloss: 17.203089 Dloss: 0.000000\n",
      "Train Epoch: 30 [Iter: 400]\t Gloss: 15.057921 Dloss: 0.000001\n",
      "Train Epoch: 30 [Iter: 500]\t Gloss: 15.906951 Dloss: 0.000000\n",
      "Train Epoch: 30 [Iter: 600]\t Gloss: 16.440710 Dloss: 0.000000\n",
      "(1, 28, 28)\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAC89JREFUeJzt3VGIpWd9x/Hvr1FvohcJIUuISddKKC25iCVIQSnbCyWVwsaLiLna0ov1woBCLxq8SaAIImp7J6QY3EKNDUTNEkpjCLbxKmQTxCRuo0G2cc2SJaRgciUx/17Mu2XczMw5O+e8531n/t8PHM4575x53/+8M795nnOe932fVBWS+vmDqQuQNA3DLzVl+KWmDL/UlOGXmjL8UlOGX2rK8EtNGX6pqfdscmNJPJxQGllVZZnXrdTyJ7kjyUtJXk5y7yrrkrRZ2e+x/UmuAn4OfAI4DzwD3F1VP9vje2z5pZFtouX/KPByVf2yqn4LfBc4vsL6JG3QKuG/EfjVtufnh2W/J8nJJGeSnFlhW5LWbJUP/HbqWryrW19VDwAPgN1+aU5WafnPAzdte/5B4NXVypG0KauE/xngliQfSvI+4LPA6fWUJWls++72V9XbSe4BHgeuAh6sqhfXVpmkUe17qG9fG/M9vzS6jRzkI+ngMvxSU4ZfasrwS00Zfqkpwy81Zfilpgy/1JThl5oy/FJThl9qyvBLTRl+qSnDLzVl+KWmDL/UlOGXmjL8UlOGX2rK8EtNGX6pqY1O0a3DZ9HVn5OlLiSrCdjyS00Zfqkpwy81Zfilpgy/1JThl5oy/FJTK43zJzkHvAn8Dni7qm5fR1HanFXH6R3HP7jWcZDPX1bV62tYj6QNstsvNbVq+Av4YZJnk5xcR0GSNmPVbv/HqurVJNcDTyT576p6avsLhn8K/mOQZiaLPvBZekXJ/cBbVfW1PV6zno1pbTwx5/CpqqV+afvu9ie5OskHLj0GPgm8sN/1SdqsVbr9R4DvDy3De4DvVNV/rKUqSaNbW7d/qY3Z7d+4Vbv1Y/59+JZiHKN3+yUdbIZfasrwS00Zfqkpwy81Zfilprx0d3OrDuVNOVSo1djyS00Zfqkpwy81Zfilpgy/1JThl5oy/FJTjvNvwNin1Y55auycx/G9CtFqbPmlpgy/1JThl5oy/FJThl9qyvBLTRl+qSnH+Tdg7HH8vb5/1bHuMWsbe3rwMffLYWDLLzVl+KWmDL/UlOGXmjL8UlOGX2rK8EtNLRznT/Ig8NfAxaq6dVh2LfBvwFHgHPCZqvrf8cqct7HPaV9l/XM+533sawXstf4575dNWabl/zZwx2XL7gWerKpbgCeH55IOkIXhr6qngDcuW3wcODU8PgXcuea6JI1sv+/5j1TVBYDh/vr1lSRpE0Y/tj/JSeDk2NuRdGX22/K/luQGgOH+4m4vrKoHqur2qrp9n9uSNIL9hv80cGJ4fAJ4dD3lSNqULDHk8RBwDLgOeA24D/gB8DBwM/AKcFdVXf6h4E7rOpTzNR/kaajnfLqxQ337U1VLFb8w/Ot0WMO/yJh/xIvWP/Yf8SohGvtv7yAHeBXLht8j/KSmDL/UlOGXmjL8UlOGX2rK8EtNeenuJa0ynDb2kNOch9MO6+WzD8NxArb8UlOGX2rK8EtNGX6pKcMvNWX4paYMv9SU4/xLGnPcdsxTfg/zKb1jXg9g6mM3NsGWX2rK8EtNGX6pKcMvNWX4paYMv9SU4Zeacpx/A6Y893vq887HHGsf+/sPO1t+qSnDLzVl+KWmDL/UlOGXmjL8UlOGX2pqYfiTPJjkYpIXti27P8mvk/xkuH1q3DKnV1W73sZc9zLrH7O2VSXZ921Vc94vc7BMy/9t4I4dlv9jVd023P59vWVJGtvC8FfVU8AbG6hF0gat8p7/niQ/Hd4WXLO2iiRtxH7D/03gw8BtwAXg67u9MMnJJGeSnNnntiSNIMt8+JHkKPBYVd16JV/b4bUH9pOWVSacPMgXqlxk6hOH9nJYJwldpKqW+uH21fInuWHb008DL+z2WknztPCU3iQPAceA65KcB+4DjiW5DSjgHPC5EWuUNIKluv1r29gB7vav4qB2jddhyuv272XKbY9t1G6/pIPP8EtNGX6pKcMvNWX4paYMv9SUl+5e0pyPFpvyCL9FxtxvqwyhTv07mwNbfqkpwy81Zfilpgy/1JThl5oy/FJThl9qynH+Jc15Gu0xT209yNNkz/nYjDmw5ZeaMvxSU4ZfasrwS00Zfqkpwy81ZfilphznPwDmPCPPKtsf+/gGp+Lemy2/1JThl5oy/FJThl9qyvBLTRl+qSnDLzW1MPxJbkryoyRnk7yY5AvD8muTPJHkF8P9NeOXq50k2fW2yvfO/Zz3qtrzdlB/rk1ZpuV/G/i7qvoT4M+Bzyf5U+Be4MmqugV4cngu6YBYGP6qulBVzw2P3wTOAjcCx4FTw8tOAXeOVaSk9bui9/xJjgIfAZ4GjlTVBdj6BwFcv+7iJI1n6WP7k7wfeAT4YlX9Ztn3TUlOAif3V56ksWSZkx+SvBd4DHi8qr4xLHsJOFZVF5LcAPxnVf3xgvW0PNNi7BNMpry46CJjntgz9sVHD6qqWuoHX+bT/gDfAs5eCv7gNHBieHwCePRKi5Q0nYUtf5KPAz8GngfeGRZ/ia33/Q8DNwOvAHdV1RsL1jXbln/KVmTOLdiYLb/GsWzLv1S3f10M//y2vYjhP3jW1u2XdDgZfqkpwy81Zfilpgy/1JThl5ry0t2DOR8lN+a2D/IU3FqNLb/UlOGXmjL8UlOGX2rK8EtNGX6pKcMvNXVoxvnnfFps123D3r8Xr9QzLVt+qSnDLzVl+KWmDL/UlOGXmjL8UlOGX2rKS3dLh4yX7pa0J8MvNWX4paYMv9SU4ZeaMvxSU4Zfamph+JPclORHSc4meTHJF4bl9yf5dZKfDLdPjV+upHVZeJBPkhuAG6rquSQfAJ4F7gQ+A7xVVV9bemMe5CONbtmDfBZeyaeqLgAXhsdvJjkL3LhaeZKmdkXv+ZMcBT4CPD0suifJT5M8mOSaXb7nZJIzSc6sVKmktVr62P4k7wf+C/hyVX0vyRHgdaCAf2DrrcHfLliH3X5pZMt2+5cKf5L3Ao8Bj1fVN3b4+lHgsaq6dcF6DL80srWd2JOtS6R+Czi7PfjDB4GXfBp44UqLlDSdZT7t/zjwY+B54J1h8ZeAu4Hb2Or2nwM+N3w4uNe6bPmlka21278uhl8an+fzS9qT4ZeaMvxSU4ZfasrwS00Zfqkpwy81Zfilpgy/1JThl5oy/FJThl9qyvBLTRl+qamFF/Bcs9eB/9n2/Lph2RzNtba51gXWtl/rrO0Pl33hRs/nf9fGkzNVdftkBexhrrXNtS6wtv2aqja7/VJThl9qaurwPzDx9vcy19rmWhdY235NUtuk7/klTWfqll/SRCYJf5I7kryU5OUk905Rw26SnEvy/DDz8KRTjA3ToF1M8sK2ZdcmeSLJL4b7HadJm6i2WczcvMfM0pPuu7nNeL3xbn+Sq4CfA58AzgPPAHdX1c82WsgukpwDbq+qyceEk/wF8BbwL5dmQ0ryVeCNqvrK8I/zmqr6+5nUdj9XOHPzSLXtNrP03zDhvlvnjNfrMEXL/1Hg5ar6ZVX9FvgucHyCOmavqp4C3rhs8XHg1PD4FFt/PBu3S22zUFUXquq54fGbwKWZpSfdd3vUNYkpwn8j8Kttz88zrym/C/hhkmeTnJy6mB0cuTQz0nB//cT1XG7hzM2bdNnM0rPZd/uZ8Xrdpgj/TrOJzGnI4WNV9WfAXwGfH7q3Ws43gQ+zNY3bBeDrUxYzzCz9CPDFqvrNlLVst0Ndk+y3KcJ/Hrhp2/MPAq9OUMeOqurV4f4i8H223qbMyWuXJkkd7i9OXM//q6rXqup3VfUO8M9MuO+GmaUfAf61qr43LJ583+1U11T7bYrwPwPckuRDSd4HfBY4PUEd75Lk6uGDGJJcDXyS+c0+fBo4MTw+ATw6YS2/Zy4zN+82szQT77u5zXg9yUE+w1DGPwFXAQ9W1Zc3XsQOkvwRW609bJ3x+J0pa0vyEHCMrbO+XgPuA34APAzcDLwC3FVVG//gbZfajnGFMzePVNtuM0s/zYT7bp0zXq+lHo/wk3ryCD+pKcMvNWX4paYMv9SU4ZeaMvxSU4ZfasrwS039Hz8+eUdCNSlvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 31 [Iter: 100]\t Gloss: 16.795523 Dloss: 0.000000\n",
      "Train Epoch: 31 [Iter: 200]\t Gloss: 17.166512 Dloss: 0.000000\n",
      "Train Epoch: 31 [Iter: 300]\t Gloss: 17.423182 Dloss: 0.000000\n",
      "Train Epoch: 31 [Iter: 400]\t Gloss: 17.491484 Dloss: 0.000000\n",
      "Train Epoch: 31 [Iter: 500]\t Gloss: 17.688023 Dloss: 0.000000\n",
      "Train Epoch: 31 [Iter: 600]\t Gloss: 17.902464 Dloss: 0.000000\n",
      "Train Epoch: 32 [Iter: 100]\t Gloss: 17.991585 Dloss: 0.000000\n",
      "Train Epoch: 32 [Iter: 200]\t Gloss: 18.134035 Dloss: 0.000000\n",
      "Train Epoch: 32 [Iter: 300]\t Gloss: 18.262941 Dloss: 0.000000\n",
      "Train Epoch: 32 [Iter: 400]\t Gloss: 18.359348 Dloss: 0.000000\n",
      "Train Epoch: 32 [Iter: 500]\t Gloss: 18.414549 Dloss: 0.000000\n",
      "Train Epoch: 32 [Iter: 600]\t Gloss: 18.517790 Dloss: 0.000000\n",
      "Train Epoch: 33 [Iter: 100]\t Gloss: 18.578817 Dloss: 0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8253151c7e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# torch.cuda.set_device(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-e2c45a273799>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, data_loader, epochs, log_interval)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_output_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# torch.cuda.set_device(1)\n",
    "_ = train(g_model, d_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
